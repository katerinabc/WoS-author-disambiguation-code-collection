{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting of disambiguate function for my data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"C:/Users/0122564s/OneDrive - National University of Ireland, Galway/SFI_project/bibliometric/analysis/biblio_tm\")\n",
    "file_path = os.path.join(DATA_DIR, 'tm_df_raw.csv')\n",
    "dat =  pd.read_csv(file_path, encoding='latin1')\n",
    "sample_author_lists = load_pubs(os.path.join(DATA_DIR, 'authors.csv'))\n",
    "all_names = extract_authors(sample_author_lists, method=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def load_pubs(csv, pmid_column='PMID', author_column='Authors', encoding='latin1'):\n",
    "    \"\"\"Takes input CSV plus options, returns a pandas Series of PMIDs linked to lists of authors\"\"\"\n",
    "    sample_pubs = pd.read_csv(file_path, encoding='latin1')\n",
    "    author_lists = sample_pubs['AU']\n",
    "    author_lists = author_lists.str.split(\";\")\n",
    "    return(author_lists)\n",
    "\n",
    "def extract_authors(author_lists, method=\"all\"):\n",
    "    names = set()\n",
    "    for auth_list in author_lists:\n",
    "        if method == \"all\":\n",
    "            auth_list = auth_list\n",
    "        elif method ==\"ends\":\n",
    "            auth_list = [auth_list[0], auth_list[-1]]\n",
    "        elif method ==\"first\":\n",
    "            auth_list = [auth_list[0]]\n",
    "        elif method ==\"last\":\n",
    "            auth_list = [auth_list[-1]]\n",
    "        \n",
    "        for auth in auth_list:\n",
    "            if len(auth) > 2:\n",
    "                names.add(auth)\n",
    "    return(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(aName):\n",
    "        import numpy as np \n",
    "    ''' This function accepts a <name hash> and using the fast community detection algorithm creates groups of names\n",
    "        that are likely to be the same author. It then either enters them in the database or writes their info to files\n",
    "        for verification\n",
    "        \n",
    "        CHANGE THIS INTO: \n",
    "        This function accepts a pandas data frame exctracted from WoS, and modified into a biblio data frame using R bibiometrixs package\n",
    "    '''\n",
    "    # split authors in AU column\n",
    "    def explode_str(df, col, sep): # splits a string in a column at the separator and copies the content for each row\n",
    "        s = df[col]\n",
    "        i = np.arange(len(s)).repeat(s.str.count(sep) + 1)\n",
    "        return df.iloc[i].assign(**{col: sep.join(s).split(sep)})\n",
    "   \n",
    "\n",
    "    # aName contains space. E.g. 'adams j'. This is how names are stored in the databse\n",
    "    # aNamePrint is the stripped version that contains only alpha-num chars. 'adamsj'\n",
    "    aNamePrint = aName[:-2]+aName[-1]\n",
    "    patt = re.compile('[\\W_]+')\n",
    "    aNamePrint = patt.sub('',aNamePrint)\n",
    "\n",
    "    ### For Debugging ###\n",
    "    open(\"%s.debug\"%aNamePrint,'a').close()\n",
    "    ### End Debug ###\n",
    "    \n",
    "    data=explode_str(aName, 'AU', ';')\n",
    "    itemA = data.to_dict('index') # transform pandas data frame into a dictionary with author as key\n",
    "    '''\n",
    "    conn=psycopg2.connect(database=\"PubData\",port=5433)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT B.author_id, A.item_id FROM Item_author A INNER JOIN Author_Hash B ON A.author_id=B.author_id WHERE name_hash=(%s);\",(aName,))\n",
    "    #print 'No. of Author names:', cur.rowcount\n",
    "    bucket_size = cur.rowcount\n",
    "    for row in cur.fetchall(): itemA[row[0]]=row[1]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemA = data.to_dict(orient='dict') # this creates a dictionary using column headers as keys\n",
    "itemA2 = data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Unnamed: 0', 'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'ID', 'AB', 'C1', 'RP', 'EM', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA', 'SN', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'PG', 'WC', 'SC', 'GA', 'UT', 'DA', 'ER', 'EI', 'SU', 'DE', 'OI', 'PM', 'RI', 'SI', 'OA', 'CT', 'CY', 'CL', 'SP', 'DB', 'AU_UN', 'AU1_UN', 'AU_UN_NR', 'SR'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemA1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-51d8b38f78f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitemA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitemA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AU'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "type(itemA['AU'])\n",
    "#list(itemA['AU']),itemA.values()\n",
    "\n",
    "result=list()\n",
    "for key in itemA:\n",
    "    for item in itemA[key]:\n",
    "        result.append(item['AU']) # does nto work!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tethne'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-3f8a60cb684d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtethne\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthethne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tethne'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import tethne as nt\n",
    "from thethne.readers import wos\n",
    "\n",
    "corpus = wos.read(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = nt.authors.coauthors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Fetching information about the authors and storing them in data structures\n",
    "    iAuthor, iAddress, iItem, iIssue, iCoAuthor = dict(), dict(), dict(), dict(), dict()\n",
    "    # Item is title of article\n",
    "    # issue is name of publication\n",
    "    for author in itemA.keys():\n",
    "        # Information related to the author\n",
    "        data = list()\n",
    "        data = itemA['AU']\n",
    "        cur.execute(\"SELECT init2,suffix,first_name FROM Author_Hash A INNER JOIN Author_Name B ON A.author_id=B.author_id WHERE A.author_id=(%s);\",(author,))\n",
    "        if cur.rowcount == 0 : data = [' ',' ',' ']\n",
    "        else:\n",
    "            temp = cur.fetchone()\n",
    "            for i in temp:\n",
    "                if i is None: data.append(' ')\n",
    "                else: data.append(i)\n",
    "        # Extract second initial from fullname\n",
    "        cur.execute(\"SELECT fullname FROM Author WHERE author_id=(%s);\",(author,))\n",
    "        if cur.rowcount == 0 : \"Author %s does not show up. This should not be happening.\"\n",
    "        else: \n",
    "            fullname = cur.fetchone()[0]\n",
    "            initials = fullname.split(',')[1].strip()\n",
    "            if len(initials) > 1:\n",
    "                data[0] = initials[1].lower()\n",
    "        # Author keywords\n",
    "        cur.execute(\"SELECT author_keyword FROM Author_Keyword WHERE author_id=(%s);\",(author,))\n",
    "        keywords = set()\n",
    "        if cur.rowcount > 0 :\n",
    "            for t in cur.fetchall() : keywords.add(t[0])\n",
    "        ai = sm.AuthorInfo(data[0],data[1],data[2],keywords)\n",
    "        iAuthor[author] = ai\n",
    "\n",
    "        # Information related to the Address\n",
    "        cur.execute(\"SELECT full_address,email FROM Author_Address A INNER JOIN Address B ON A.address_id = B.address_id WHERE A.author_id=(%s);\",(author,))\n",
    "        if cur.rowcount == 0 : data = [' ',' ']\n",
    "        else:\n",
    "            temp = cur.fetchone()\n",
    "            data = list()\n",
    "            for i in temp:\n",
    "                if i is None: data.append(' ')\n",
    "                else: data.append(i)\n",
    "        ai = sm.AddressInfo(data[0],data[1])\n",
    "        iAddress[author] = ai\n",
    "        \n",
    "        # Information related to Item\n",
    "        data = range(5)\n",
    "        cur.execute(\"SELECT title FROM Item WHERE item_id=(%s);\",(itemA[author],))\n",
    "        if cur.rowcount == 0: data[0] = ' '\n",
    "        else : data[0] = cur.fetchone()[0]\n",
    "        cur.execute(\"SELECT language FROM Item_Language WHERE item_id=(%s);\",(itemA[author],))\n",
    "        if cur.rowcount == 0: data[1] = set()\n",
    "        else : \n",
    "            tset = set()\n",
    "            for x in cur.fetchall() : tset.add(x[0])\n",
    "            data[1] = tset\n",
    "        cur.execute(\"SELECT item_keyword FROM Item_Keyword WHERE item_id=(%s);\",(itemA[author],))\n",
    "        if cur.rowcount == 0: data[2] = set()\n",
    "        else :\n",
    "            tset = set()\n",
    "            for x in cur.fetchall():tset.add(x[0])\n",
    "            data[2] = tset\n",
    "        cur.execute(\"SELECT B.org_name FROM Research_org A INNER JOIN Organization B ON A.org_id = B.org_id WHERE A.item_id=(%s);\",(itemA[author],))\n",
    "        if cur.rowcount == 0: data[3] = set()\n",
    "        else :\n",
    "            tset = set()\n",
    "            for x in cur.fetchall() : tset.add(x[0])\n",
    "            data[3] = tset\n",
    "        cur.execute(\"SELECT pref_name FROM Item_PrefName WHERE item_id=(%s);\",(itemA[author],))\n",
    "        if cur.rowcount == 0 : data[4] = set()\n",
    "        else :\n",
    "            tset = set()\n",
    "            for x in cur.fetchall() : tset.add(x[0])\n",
    "            data[4] = tset\n",
    "\n",
    "        ii = sm.ItemInfo(data[0],data[1],data[2],data[3],data[4])\n",
    "        iItem[author] = ii     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Information about Issue\n",
    "        cur.execute('''SELECT D.full_title, D.subject, D.year FROM Item A INNER JOIN \n",
    "                       (SELECT B.issue_id, B.full_title, C.subject, B.year  FROM \n",
    "                       Issue B INNER JOIN Subject_Cat C ON B.issue_id=C.issue_id) \n",
    "                       AS D ON A.issue_id=D.issue_id WHERE item_id=(%s);''',(itemA[author],))\n",
    "        data = range(3)\n",
    "        if cur.rowcount == 0: data = [' ',set(),' '] \n",
    "        else : \n",
    "            tset = set()\n",
    "            temp = cur.fetchone()\n",
    "            data[0] = temp[0]\n",
    "            tset.add(temp[1])\n",
    "            data[2] = int(temp[2])\n",
    "            for x in cur.fetchall(): tset.add(x[1])\n",
    "            data[1] = tset\n",
    "        iIssue[author] = sm.IssueInfo(data[0],data[1],data[2])\n",
    "\n",
    "        # Information about CoAuthors\n",
    "        tset = set()\n",
    "        cur.execute('''SELECT name_hash FROM\n",
    "                       Item_Author A INNER JOIN Author_Hash B\n",
    "                       ON A.author_id=B.author_id\n",
    "                       WHERE A.item_id=(%s)\n",
    "                       AND NOT A.author_id=(%s);''',(itemA[author],author))    \n",
    "        if cur.rowcount > 0:\n",
    "            for x in cur.fetchall() : tset.add(x[0])\n",
    "        tset.discard(aName)\n",
    "        iCoAuthor[author] = sm.CoAuthorInfo(tset)\n",
    "\n",
    "#    for key in itemA.keys(): \n",
    "#        print key, iIssue[key].title\n",
    "#        print iIssue[key].subject\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compute edge scores\n",
    "    if bucket_size > 1000 :\n",
    "        x = (bucket_size - 1000.0)/1000.0\n",
    "        a, b = 7.0/552, 79.0/552 \n",
    "        cutoff = ceil(a*x*x + b*x + 5)\n",
    "    elif bucket_size <= 20:\n",
    "        cutoff = 3\n",
    "    elif bucket_size <= 100:\n",
    "        cutoff = 4\n",
    "    else:\n",
    "        cutoff = 5\n",
    "    #print '%s cutoff = %d'%(aNamePrint,cutoff)\n",
    "    edges_exist = False\n",
    "    wpairs = dict()\n",
    "    cite_set = set() # Stores pair of authors with citation between them\n",
    "    coauthor_set = set() # Stores pair of authors that share coauthors with same last name first initial\n",
    "    negative = list() # Stores negative edges\n",
    "    # singles contain nodes not present in the graph\n",
    "    singles = set(itemA.keys())\n",
    "    for a in comb(itemA.keys(),2):\n",
    "        au_score = sm.authorScore(iAuthor[a[0]],iAuthor[a[1]])\n",
    "        ad_score = sm.addressScore(iAddress[a[0]],iAddress[a[1]])\n",
    "        it_score = sm.itemScore(iItem[a[0]],iItem[a[1]])\n",
    "        is_score = sm.issueScore(iIssue[a[0]],iIssue[a[1]])\n",
    "        co_score = sm.coAuthorScore(iCoAuthor[a[0]],iCoAuthor[a[1]])\n",
    "        if co_score > 0 : coauthor_set.add((a[0],a[1]))\n",
    "        ci_score = sm.citeScore(itemA[a[0]],itemA[a[1]],cur)\n",
    "        if ci_score > 0 : cite_set.add((a[0],a[1]))\n",
    "        in_score = sm.interaction(iItem[a[0]],iItem[a[1]],iIssue[a[0]],iIssue[a[1]],iCoAuthor[a[0]],iCoAuthor[a[1]])\n",
    "        score = au_score + ad_score + it_score + is_score + co_score + ci_score + in_score\n",
    "        # Change : subtracting cutoff from edge weight\n",
    "        wpairs[(a[0],a[1])] = score - cutoff\n",
    "        if score < 0 : negative.append([a[0],a[1]])\n",
    "        if score > cutoff : \n",
    "            singles.difference_update([a[0],a[1]])\n",
    "            edges_exist = True\n",
    "\n",
    "    # If no edge exists, skip rest of disambiguate function\n",
    "    # and call individual()\n",
    "    if not edges_exist:\n",
    "        for author in singles:\n",
    "            au = iAuthor[author]\n",
    "            f, s = name(au)\n",
    "            aff = {iIssue[author].year:iItem[author].pref_name.pop()}\n",
    "            grp = set([author])\n",
    "            individual(cur,grp,aff,f,s,aName)\n",
    "\n",
    "        # Insert completed hashes in Hash_Done\n",
    "        cur.execute(\"INSERT INTO Hash_Done (name_hash) VALUES (%s);\",(aName,))            \n",
    "        return_handle(cur,aNamePrint,2) \n",
    "        return\n",
    "        # disambiguate() ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Map node ids to sequential numbers. Helps increase computation speed immensely\n",
    "    # Also write new edge file to be used by fast community algorithm\n",
    "    i = 1\n",
    "    f = open('%s.wpairs'%aNamePrint,'w')\n",
    "    nodemap = dict()\n",
    "    for e in wpairs.keys():\n",
    "        if wpairs[e] > 0 :\n",
    "            n1,n2,w = e[0],e[1],wpairs[e]\n",
    "            if n1 in nodemap : n1 = nodemap[n1]\n",
    "            else : \n",
    "                nodemap[n1] = i\n",
    "                n1 = i\n",
    "                i += 1\n",
    "            if n2 in nodemap : n2 = nodemap[n2]\n",
    "            else : \n",
    "                nodemap[n2] = i\n",
    "                n2 = i\n",
    "                i += 1\n",
    "            f.write('%d\\t%d\\t%d\\n'%(n1,n2,w))\n",
    "    f.close() # Edge file written\n",
    "\n",
    "    # Reverse mapping dictionary\n",
    "    revmap = dict()\n",
    "    for k in nodemap.keys() : revmap[nodemap[k]] = k\n",
    "\n",
    "    # Running fast community algorithm\n",
    "    command = \"sudo /media/RaidDisk1/disamb/fast_comm/FastCommunity_wMH -f %s.wpairs -l 1\"%aNamePrint\n",
    "    try:\n",
    "        p = subprocess.Popen(command,shell=True)\n",
    "        p.wait() # Waiting for algorithm to finish\n",
    "    except OSError as e:\n",
    "        print(e.strerror) \n",
    "        print('except', aNamePrint)\n",
    "        return_handle(cur, aNamePrint, 1)\n",
    "        return\n",
    "\n",
    "    # Reading group file written by fast_community algorithm\n",
    "    group_file = '%s-fc_1.groups'%aNamePrint\n",
    "    gs, groups = list(),list()\n",
    "    g = set()\n",
    "    gdict= dict()\n",
    "    code = -1 #Code used to remove files at the end\n",
    "    try:\n",
    "        # Opening group file\n",
    "        gfile = open(group_file,'r')\n",
    "    except:\n",
    "        # Groups file does not exist\n",
    "        # Perform simple grouping based on connected components\n",
    "        simple_cutoff = 10 - cutoff\n",
    "        G = dict()\n",
    "        for e in wpairs.keys():\n",
    "            n1, n2, w = e[0], e[1], wpairs[e]\n",
    "            if not n1 in G : G[n1] = set()\n",
    "            if not n2 in G : G[n2] = set()\n",
    "            if w > simple_cutoff:\n",
    "                G[n1].add(n2)\n",
    "                G[n2].add(n1)\n",
    "        gs = components(G)\n",
    "        code = 1        \n",
    "    else:\n",
    "        # Groups file exists\n",
    "        for line in gfile.readlines():\n",
    "            if line[:5] == 'GROUP' :\n",
    "                if len(g) > 0 : gs.append(g)\n",
    "                g = set()\n",
    "            else :\n",
    "                node = revmap[int(line.split()[0])] \n",
    "                g.add(node)\n",
    "        gs.append(g)\n",
    "        gfile.close()\n",
    "        code = 0\n",
    "\n",
    "    # Generic group handling begins\n",
    "    counter = 0\n",
    "    for g in gs:\n",
    "        if len(g) == 1: singles.update(g)\n",
    "        else: \n",
    "            new_g = set()\n",
    "            for node in g:\n",
    "                gdict[node] = counter\n",
    "                new_g.add(node)\n",
    "            groups.append(new_g)\n",
    "            counter += 1 \n",
    "            \n",
    "    # Assigning group numbers to singletons\n",
    "    for node in singles:\n",
    "        if not node in gdict:\n",
    "            counter += 1\n",
    "            gdict[node] = counter\n",
    "\n",
    "#---Removing negative edge weights from within a group\n",
    "    for e in negative:\n",
    "        if gdict[e[0]] == gdict[e[1]]:\n",
    "            group_no = gdict[e[0]]\n",
    "            rnode = removeNode(e,groups[group_no],wpairs)\n",
    "            groups[group_no].discard(rnode)\n",
    "            singles.add(rnode)\n",
    "            counter += 1\n",
    "            gdict[rnode] = counter\n",
    "\n",
    "#---Post clustering group merge\n",
    "    iGroup = list()\n",
    "    # Create group info for pairwise comparison\n",
    "    for i in xrange(len(groups)):\n",
    "        f_name, s_init = list(), list() #First Name and Second initial\n",
    "        address = set()\n",
    "        for author in groups[i]:\n",
    "            # Name\n",
    "            au = iAuthor[author]\n",
    "            f, s = name(au)\n",
    "            if not f == '': f_name.append(f)\n",
    "            if not s == '':s_init.append(s)\n",
    "            # Address    \n",
    "            ad = iAddress[author]\n",
    "            if ad.fullAdd[-4:] == 'USA': \n",
    "                c,s,p = sm.usAddress(ad.fullAddress.lower())\n",
    "                address.add((c,s))\n",
    "        if len(f_name) == 0: first = ''\n",
    "        else : first = Counter(f_name).most_common(1)[0][0]\n",
    "        if len(s_init) == 0: init2 = ''\n",
    "        else : init2 = Counter(s_init).most_common(1)[0][0]\n",
    "        aff = affiliation(cur,groups[i],iItem,iIssue,iAddress)\n",
    "        ginfo = sm.GroupInfo(i,(first,init2),address,aff)\n",
    "        iGroup.append(ginfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create graph to merge connected components\n",
    "    G = dict() # Adjacency list\n",
    "    for i in xrange(len(groups)): G[i] = set()\n",
    "    for g in comb(iGroup,2):\n",
    "        # Group Number\n",
    "        i1, i2 = g[0].i, g[1].i\n",
    "        # Name\n",
    "        name_score, addr_score, cite_score, co_score, aff_score = 0,0,0,0,0\n",
    "        f1, s1 = g[0].name\n",
    "        f2, s2 = g[1].name\n",
    "        if f1 == '' or f2 == '' : pass\n",
    "        elif f1 == f2 : name_score += 5\n",
    "        else : continue\n",
    "        if s1 == '' or s2 == '' : pass\n",
    "        elif s1 == s2 : name_score += 5\n",
    "        else : continue\n",
    "        if name_score < 10 : name_score = 0 \n",
    "        # Address\n",
    "        if len(g[0].address & g[1].address) > 0 : addr_score = 10\n",
    "        # Citation and CoAuthor\n",
    "        cite_found, coauthor_found = False, False \n",
    "        for x,y in prod(groups[i1], groups[i2]):\n",
    "            if (x,y) in cite_set or (y,x) in cite_set:\n",
    "                cite_found = True\n",
    "                cite_score = 10\n",
    "                if coauthor_found: break\n",
    "            if (x,y) in coauthor_set or (y,x) in coauthor_set:\n",
    "                coauthor_found = True\n",
    "                co_score = 10\n",
    "                if cite_found: break        \n",
    "        # Affiliation\n",
    "        aff1, aff2 = g[0].affiliation, g[1].affiliation\n",
    "        for y in aff1.keys():\n",
    "            if aff1[y] == aff2.get(y,'NIL'):\n",
    "                aff_score = 10\n",
    "                break\n",
    "        score = name_score+addr_score+cite_score+co_score+aff_score\n",
    "        #print 'Groups', i1, i2\n",
    "        #print 'Name:', name_score, 'Address:', addr_score, 'Cite:', cite_score, \\\n",
    "        #      'CoAuthor:', co_score, 'Affiliation:', aff_score, 'Total:', score\n",
    "        if score >= 20:\n",
    "            G[i1].add(i2)\n",
    "            G[i2].add(i1)\n",
    "\n",
    "    # Forming new, possibly merged, groups \n",
    "    group_list = components(G)\n",
    "    #print 'group_list:', group_list\n",
    "    new_groups = list()\n",
    "    for gl in group_list:\n",
    "        g = set()\n",
    "        for x in gl: g.update(groups[x])\n",
    "        new_groups.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''        \n",
    "#############################################################\n",
    "# Printing group information for manual curation\n",
    "    #Add the singles group to the end\n",
    "    new_groups.append(singles)\n",
    "    # Make directory for author hash and store groups inside\n",
    "    os.mkdir(aNamePrint)\n",
    "    i = 0\n",
    "    for group in new_groups:\n",
    "        file = ''\n",
    "        file = open(\"%s/Group%s.txt\"%(aNamePrint,i),'w')\n",
    "        i += 1\n",
    "        file.write(\"Author_id\\tSec Initial\\tSuffix\\tFirst\\tKeywords\\tCo Authors\\tAddress\\tEmail\\tItem Title\\tLanguage\\tItem Keywords\\tOrganizations\\tPref Names\\tYear\\tIssue Title\\tIssue Subjects \\n\")\n",
    "        for author in group:\n",
    "            au = iAuthor[author]\n",
    "            ad = iAddress[author]\n",
    "            it = iItem[author]\n",
    "            iu = iIssue[author]\n",
    "            co = iCoAuthor[author]\n",
    "            keywords = ', '.join(au.keywords)\n",
    "            au_string = \" %s\\t%s\\t%s\\t%s\"%(au.init2,au.suffix,au.first,keywords)\n",
    "            ad_string = \" %s\\t%s\"%(ad.fullAdd,ad.email)\n",
    "            language = ', '.join(it.lang)\n",
    "            keywords = ', '.join(it.keywords)\n",
    "            org = ', '.join(it.org)\n",
    "            pref_name = ', '.join(it.pref_name)\n",
    "            it_string = \" %s\\t%s\\t%s\\t%s\\t%s\"%(it.title,language,keywords,org,pref_name)\n",
    "            subject = ', '.join(iu.subject)\n",
    "            iu_string = \" %s\\t%s\\t%s\"%(iu.year,iu.title,subject)\n",
    "            co_string = ', '.join(co.authors)\n",
    "            file.write(\"%d\\t%s\\t%s\\t%s\\t%s\\t%s\\n\"%(author,au_string,co_string,ad_string,it_string,iu_string))\n",
    "        file.close()\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "#---Populate Individual details using individual function\n",
    "    for ng in new_groups:\n",
    "        f_name, s_init = list(), list() #First Name and Second initial\n",
    "        for author in ng:\n",
    "            # Name\n",
    "            au = iAuthor[author]\n",
    "            f, s = name(au)\n",
    "            if not f == '': f_name.append(f)\n",
    "            if not s == '': s_init.append(s)\n",
    "        if len(f_name) == 0: first = ''\n",
    "        else : first = Counter(f_name).most_common(1)[0][0]\n",
    "        if len(s_init) == 0: init2 = ''\n",
    "        else : init2 = Counter(s_init).most_common(1)[0][0]\n",
    "        aff = affiliation(cur,ng,iItem,iIssue,iAddress)\n",
    "        #print ' '.join(['%d:%s'%(y,aff[y])for y in sorted(aff.keys())]) \n",
    "\n",
    "        individual(cur,ng,aff,first,init2,aName)\n",
    "\n",
    "    for author in singles:\n",
    "        au = iAuthor[author]\n",
    "        f, s = name(au)\n",
    "        aff = {iIssue[author].year:iItem[author].pref_name.pop()}\n",
    "        grp = set([author])\n",
    "        individual(cur,grp,aff,f,s,aName)\n",
    "\n",
    "    # Insert completed hashes in Hash_Done\n",
    "    cur.execute(\"INSERT INTO Hash_Done (name_hash) VALUES (%s);\",(aName,))                \n",
    "    return_handle(cur,aNamePrint,code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
